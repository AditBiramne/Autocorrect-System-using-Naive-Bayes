# ğŸ”  Autocorrect-System-using-Naive-Bayes

A simple and lightweight **autocorrect system** built using the **Naive Bayes approach** and **edit distance**. It simulates how intelligent text correction can be achieved without large datasets or deep learning â€” ideal for understanding core NLP concepts.

---

ğŸ“Œ Overview

This system corrects misspelled words by calculating **posterior probabilities** of candidate words using:

- **P(C)**: Prior probability (based on word frequency)
- **P(W|C)**: Likelihood (based on edit distance between input and candidates)
- **P(C|W)**: Posterior probability (product of the above two)

---

ğŸ” Example

```python
misspelled_word = "helo"
suggestions = autocorrect_naive_bayes(misspelled_word, word_counts, total_words)
```

âœ… Returns top suggestions like:

```
hello (Probability: 0.1)
world (Probability: 0.05)
data (Probability: 0.04)
```

---

ğŸ“¦ Dependencies

- Python 3
- editdistance
- collections (Python standard library)

ğŸ›  Install with:

```bash
pip install editdistance
```

---

ğŸ“ How It Works

### 1ï¸âƒ£ Sample Corpus (Unigram)

A basic vocabulary is hardcoded:

```python
corpus = [
  "hello", "world", "python", "programming",
  "machine", "learning", "artificial", "intelligence",
  "data", "science"
]
```

### 2ï¸âƒ£ Word Frequency

Frequencies are counted using:

```python
from collections import Counter
word_counts = Counter(corpus)
total_words = sum(word_counts.values())
```

### 3ï¸âƒ£ Prior Probability: P(C)

```python
prior = word_counts[candidate] / total_words
```

### 4ï¸âƒ£ Likelihood: P(W|C)

```python
import editdistance
likelihood = 1 / (editdistance.eval(input_word, candidate) + 1)
```

### 5ï¸âƒ£ Posterior: P(C|W)

```python
posterior = prior * likelihood
```

---

ğŸš€ How to Run

1. âœ… Install the required package:

```bash
pip install editdistance
```

2. âœ… Save the script as `autocorrect.py`

3. âœ… Run using:

```bash
python autocorrect.py
```

4. âœ… Output:

```
Suggestions for 'helo':
hello (Probability: 0.1)
world (Probability: 0.05)
data (Probability: 0.04)
...
```

---

ğŸŒŸ Highlights

- No external dataset required â€” simple unigram model
- Lightweight and easy to understand
- Demonstrates key ML concepts: prior, likelihood, and posterior
- Great for learning **Naive Bayes** without overwhelming complexity

---

âš™ï¸ Future Improvements

- ğŸ“š Use a **large real-world corpus** (e.g., from Wikipedia, books, or subtitles)
- ğŸ§  Incorporate **bigram** or **context-based probabilities**
- ğŸ”¤ Improve likelihood with **realistic typo models**
- ğŸ Add ranking metrics like Levenshtein weight or keyboard distance
- ğŸ“² Convert to an **interactive web app** using Flask or Streamlit

---

ğŸ“š References

- [EditDistance PyPI](https://pypi.org/project/editdistance/)
- [Naive Bayes Algorithm - Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)

